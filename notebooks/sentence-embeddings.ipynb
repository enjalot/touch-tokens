{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the most downloaded sentence transformer on HuggingFace\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"This is an example sentence\", \"Here is an example sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each input sentence gets a 384 dimension embedding array associated with it\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.76568896e-02,  6.34958670e-02,  4.87130694e-02,  7.93049484e-02,\n",
       "        3.74480374e-02,  2.65276735e-03,  3.93748544e-02, -7.09845824e-03,\n",
       "        5.93614839e-02,  3.15370038e-02,  6.00980558e-02, -5.29051572e-02,\n",
       "        4.06067856e-02, -2.59308089e-02,  2.98427846e-02,  1.12691044e-03,\n",
       "        7.35149235e-02, -5.03819846e-02, -1.22386619e-01,  2.37028450e-02,\n",
       "        2.97265202e-02,  4.24768701e-02,  2.56337672e-02,  1.99519377e-03,\n",
       "       -5.69190904e-02, -2.71598604e-02, -3.29035521e-02,  6.60248920e-02,\n",
       "        1.19007073e-01, -4.58791479e-02, -7.26214498e-02, -3.25839967e-02,\n",
       "        5.23413680e-02,  4.50552627e-02,  8.25300813e-03,  3.67023535e-02,\n",
       "       -1.39415022e-02,  6.53919503e-02, -2.64272951e-02,  2.06431461e-04,\n",
       "       -1.36643415e-02, -3.62809561e-02, -1.95043348e-02, -2.89738495e-02,\n",
       "        3.94270383e-02, -8.84090587e-02,  2.62426888e-03,  1.36713758e-02,\n",
       "        4.83063124e-02, -3.11565623e-02, -1.17329180e-01, -5.11690155e-02,\n",
       "       -8.85287523e-02, -2.18962207e-02,  1.42986467e-02,  4.44167554e-02,\n",
       "       -1.34814708e-02,  7.43392482e-02,  2.66382731e-02, -1.98762473e-02,\n",
       "        1.79190934e-02, -1.06052877e-02, -9.04263183e-02,  2.13269200e-02,\n",
       "        1.41204879e-01, -6.47170795e-03, -1.40387507e-03, -1.53609915e-02,\n",
       "       -8.73571262e-02,  7.22173825e-02,  2.01402996e-02,  4.25587259e-02,\n",
       "       -3.49014327e-02,  3.19619547e-04, -8.02970454e-02, -3.27472351e-02,\n",
       "        2.85268165e-02, -5.13658188e-02,  1.09389208e-01,  8.19328278e-02,\n",
       "       -9.84039977e-02, -9.34095234e-02, -1.51292263e-02,  4.51248325e-02,\n",
       "        4.94171754e-02, -2.51867846e-02,  1.57077406e-02, -1.29290745e-01,\n",
       "        5.31896483e-03,  4.02343599e-03, -2.34572198e-02, -6.72983006e-02,\n",
       "        2.92280950e-02, -2.60845404e-02,  1.30625162e-02, -3.11663486e-02,\n",
       "       -4.82713580e-02, -5.58859631e-02, -3.87505777e-02,  1.20010830e-01,\n",
       "       -1.03924731e-02,  4.89704870e-02,  5.53537086e-02,  4.49359119e-02,\n",
       "       -4.00964404e-03, -1.02959737e-01, -2.92968489e-02, -5.83402254e-02,\n",
       "        2.70472467e-02, -2.20169537e-02, -7.22241625e-02, -4.13869396e-02,\n",
       "       -1.93298031e-02,  2.73331138e-03,  2.76936655e-04, -9.67587903e-02,\n",
       "       -1.00574739e-01, -1.41922832e-02, -8.07891637e-02,  4.53925505e-02,\n",
       "        2.45041028e-02,  5.97613864e-02, -7.38185495e-02,  1.19843753e-02,\n",
       "       -6.63403347e-02, -7.69044757e-02,  3.85157391e-02, -5.59361999e-33,\n",
       "        2.80013308e-02, -5.60784861e-02, -4.86601777e-02,  2.15569865e-02,\n",
       "        6.01980798e-02, -4.81403135e-02, -3.50246876e-02,  1.93313956e-02,\n",
       "       -1.75151769e-02, -3.89210135e-02, -3.81066697e-03, -1.70287527e-02,\n",
       "        2.82099750e-02,  1.28290188e-02,  4.71601412e-02,  6.21029772e-02,\n",
       "       -6.43588677e-02,  1.29285693e-01, -1.31231304e-02,  5.23069315e-02,\n",
       "       -3.73681225e-02,  2.89094523e-02, -1.68981049e-02, -2.37330273e-02,\n",
       "       -3.33491899e-02, -5.16763069e-02,  1.55356601e-02,  2.08803080e-02,\n",
       "       -1.25371041e-02,  4.59579006e-02,  3.72720622e-02,  2.80567333e-02,\n",
       "       -5.90005480e-02, -1.16988262e-02,  4.92182337e-02,  4.70328405e-02,\n",
       "        7.35487267e-02, -3.70529853e-02,  3.98458866e-03,  1.06412210e-02,\n",
       "       -1.61566859e-04, -5.27166240e-02,  2.75927633e-02, -3.92921790e-02,\n",
       "        8.44717771e-02,  4.86860499e-02, -4.85874387e-03,  1.79948211e-02,\n",
       "       -4.28569950e-02,  1.23375673e-02,  6.39954302e-03,  4.04822677e-02,\n",
       "        1.48887197e-02, -1.53941615e-02,  7.62947723e-02,  2.37043649e-02,\n",
       "        4.45237719e-02,  5.08195199e-02, -2.31254008e-03, -1.88737046e-02,\n",
       "       -1.23335924e-02,  4.66002375e-02, -5.63438348e-02,  6.29927218e-02,\n",
       "       -3.15535180e-02,  3.24912593e-02,  2.34673414e-02, -6.55437857e-02,\n",
       "        2.01709121e-02,  2.57082656e-02, -1.23868380e-02, -8.36498570e-03,\n",
       "       -6.64377734e-02,  9.43074003e-02, -3.57092880e-02, -3.42483260e-02,\n",
       "       -6.66351616e-03, -8.01527128e-03, -3.09711453e-02,  4.33012135e-02,\n",
       "       -8.21395405e-03, -1.50795072e-01,  3.07692476e-02,  4.00718749e-02,\n",
       "       -3.79294008e-02,  1.93211203e-03,  4.00530137e-02, -8.77074599e-02,\n",
       "       -3.68492007e-02,  8.57952982e-03, -3.19251567e-02, -1.25257988e-02,\n",
       "        7.35539421e-02,  1.34738465e-03,  2.05919184e-02,  2.71097944e-33,\n",
       "       -5.18577173e-02,  5.78360707e-02, -9.18985307e-02,  3.94421965e-02,\n",
       "        1.05576567e-01, -1.96912345e-02,  6.18402883e-02, -7.63464645e-02,\n",
       "        2.40880474e-02,  9.40049514e-02, -1.16535522e-01,  3.71198617e-02,\n",
       "        5.22425324e-02, -3.95853445e-03,  5.72214462e-02,  5.32855233e-03,\n",
       "        1.24016799e-01,  1.39022470e-02, -1.10249659e-02,  3.56052667e-02,\n",
       "       -3.30754817e-02,  8.16573948e-02, -1.52003793e-02,  6.05585165e-02,\n",
       "       -6.01397529e-02,  3.26102600e-02, -3.48296724e-02, -1.69881713e-02,\n",
       "       -9.74907652e-02, -2.71483362e-02,  1.74705859e-03, -7.68982321e-02,\n",
       "       -4.31858413e-02, -1.89985614e-02, -2.91661397e-02,  5.77488281e-02,\n",
       "        2.41821613e-02, -1.16901956e-02, -6.21435009e-02,  2.84351595e-02,\n",
       "       -2.37499116e-04, -2.51783095e-02,  4.39634733e-03,  8.12840462e-02,\n",
       "        3.64184491e-02, -6.04006238e-02, -3.65517400e-02, -7.93748125e-02,\n",
       "       -5.08530438e-03,  6.69699162e-02, -1.17784359e-01,  3.23743932e-02,\n",
       "       -4.71252017e-02, -1.34459808e-02, -9.48445275e-02,  8.24948400e-03,\n",
       "       -1.06748398e-02, -6.81882203e-02,  1.11814076e-03,  2.48019770e-02,\n",
       "       -6.35889620e-02,  2.84492448e-02, -2.61303354e-02,  8.58111605e-02,\n",
       "        1.14682294e-01, -5.35345711e-02, -5.63588142e-02,  4.26009074e-02,\n",
       "        1.09453918e-02,  2.09578853e-02,  1.00131229e-01,  3.26051824e-02,\n",
       "       -1.84208825e-01, -3.93208340e-02, -6.91454634e-02, -6.38105571e-02,\n",
       "       -6.56385869e-02, -6.41251076e-03, -4.79612537e-02, -7.68133253e-02,\n",
       "        2.95384284e-02, -2.29948629e-02,  4.17037047e-02, -2.50047352e-02,\n",
       "       -4.54507675e-03, -4.17136289e-02, -1.32289445e-02, -6.38358146e-02,\n",
       "       -2.46471190e-03, -1.37337595e-02,  1.68976504e-02, -6.30397946e-02,\n",
       "        8.98881182e-02,  4.18170728e-02, -1.85687412e-02, -1.80442168e-08,\n",
       "       -1.67998504e-02, -3.21577787e-02,  6.30384162e-02, -4.13092375e-02,\n",
       "        4.44819033e-02,  2.02469318e-03,  6.29593059e-02, -5.17373765e-03,\n",
       "       -1.00444090e-02, -3.05640437e-02,  3.52672525e-02,  5.58581501e-02,\n",
       "       -4.67124954e-02,  3.45102809e-02,  3.29577811e-02,  4.30114716e-02,\n",
       "        2.94361692e-02, -3.03164162e-02, -1.71107613e-02,  7.37485290e-02,\n",
       "       -5.47910109e-02,  2.77515482e-02,  6.20164722e-03,  1.58800613e-02,\n",
       "        3.42978910e-02, -5.15753636e-03,  2.35079546e-02,  7.53135756e-02,\n",
       "        1.92843489e-02,  3.36196646e-02,  5.09103425e-02,  1.52497083e-01,\n",
       "        1.64207891e-02,  2.70528421e-02,  3.75162661e-02,  2.18553636e-02,\n",
       "        5.66334054e-02, -3.95747535e-02,  7.12313578e-02, -5.41377142e-02,\n",
       "        1.03773864e-03,  2.11852789e-02, -3.56308818e-02,  1.09017015e-01,\n",
       "        2.76526529e-03,  3.13997418e-02,  1.38424442e-03, -3.45738083e-02,\n",
       "       -4.59277779e-02,  2.88083721e-02,  7.16907391e-03,  4.84685004e-02,\n",
       "        2.61018313e-02, -9.44075454e-03,  2.82169469e-02,  3.48723605e-02,\n",
       "        3.69098298e-02, -8.58950056e-03, -3.53205763e-02, -2.47856807e-02,\n",
       "       -1.91921424e-02,  3.80707458e-02,  5.99653870e-02, -4.22286764e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8809])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine similarity between the two sentence embeddings\n",
    "# this is the foundation of all RAG stuff. basically embed your dataset\n",
    "# then embed a query and find the most similar embeddings in your dataset\n",
    "# then use the original text to answer the query\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(torch.tensor(embeddings[0]).unsqueeze(0), torch.tensor(embeddings[1]).unsqueeze(0))\n",
    "cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sentence transformers library simplifies a lot of things for making embeddings\n",
    "# let's take a look at some of the stuff that goes on under the hood\n",
    "# this will be more like the LLM stuff\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many sentence transformers models are based on the BERT architecture\n",
    "# though recently there are some based on LLMs \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
       "  \"architectures\": [\n",
       "    \"BertModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 384,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.45.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see here that hidden_size is 384, the dimensionality of the embeddings\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='sentence-transformers/all-MiniLM-L6-v2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a key variable here is model_max_length=512, so that means the model can only handle up to 512 tokens\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do a single longer input to take a closer look at what happens with tokens\n",
    "# uncomment the second input to see truncation happen (its 233 * 3 tokens)\n",
    "# taken from this list of top 10 paragraphs https://www.jjrlore.com/post/top-10-paragraphs\n",
    "inputs = [\n",
    "  \"\"\"\n",
    "  Books bombarded his shoulder, his arms, his upturned face.  A book lit, almost obediently, like a white pigeon, in his hands, wings fluttering.  In the dim, wavering light, a page hung open and it was like a snowy feather, the words delicately painted thereon.  In all the rush and fervor, Montage had only an instant to read a line, but it blazed in his mind for the next minute as if stamped there with fiery steel.  “Time has fallen asleep in the afternoon sunshine.”  He dropped the book.  Immediately, another fell into his arms.\n",
    "  \"\"\",\n",
    "  # \"\"\"\n",
    "  # The tent he lived in stood right smack up against the wall of the shallow, dull-colored forest separating his own squadron from Dunbar’ s.  Immediately alongside was the abandoned railroad ditch that carried the pipe that carried the aviation gasoline down to the fuel trucks at the airfield.  Thanks to Orr, his roommate, it was the most luxurious tent in the squadron.  Each time Yossarian returned from one of his holidays in the hospital or rest leaves in Rome, he was surprised by some new comfort Orr had installed in his absence - running water, wood-burning fireplace, cement floor.  Yossarian had chosen the site, and he and Orr had raised the tent to get her.  Orr, who was a grinning pygmy with pilot’s wings and thick, wavy brown hair parted in the middle, furnished all the knowledge, while Yossarian, who was taller, stronger, broader, and faster, did most of the work.  Just the two of them lived there, although the tent was big enough for six.  When summer came, Orr rolled up the side flaps to allow a breeze that never blew to flush away the air baking inside.\n",
    "  # The tent he lived in stood right smack up against the wall of the shallow, dull-colored forest separating his own squadron from Dunbar’ s.  Immediately alongside was the abandoned railroad ditch that carried the pipe that carried the aviation gasoline down to the fuel trucks at the airfield.  Thanks to Orr, his roommate, it was the most luxurious tent in the squadron.  Each time Yossarian returned from one of his holidays in the hospital or rest leaves in Rome, he was surprised by some new comfort Orr had installed in his absence - running water, wood-burning fireplace, cement floor.  Yossarian had chosen the site, and he and Orr had raised the tent to get her.  Orr, who was a grinning pygmy with pilot’s wings and thick, wavy brown hair parted in the middle, furnished all the knowledge, while Yossarian, who was taller, stronger, broader, and faster, did most of the work.  Just the two of them lived there, although the tent was big enough for six.  When summer came, Orr rolled up the side flaps to allow a breeze that never blew to flush away the air baking inside.\n",
    "  # The tent he lived in stood right smack up against the wall of the shallow, dull-colored forest separating his own squadron from Dunbar’ s.  Immediately alongside was the abandoned railroad ditch that carried the pipe that carried the aviation gasoline down to the fuel trucks at the airfield.  Thanks to Orr, his roommate, it was the most luxurious tent in the squadron.  Each time Yossarian returned from one of his holidays in the hospital or rest leaves in Rome, he was surprised by some new comfort Orr had installed in his absence - running water, wood-burning fireplace, cement floor.  Yossarian had chosen the site, and he and Orr had raised the tent to get her.  Orr, who was a grinning pygmy with pilot’s wings and thick, wavy brown hair parted in the middle, furnished all the knowledge, while Yossarian, who was taller, stronger, broader, and faster, did most of the work.  Just the two of them lived there, although the tent was big enough for six.  When summer came, Orr rolled up the side flaps to allow a breeze that never blew to flush away the air baking inside.\n",
    "  # \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 127])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "# basically we average the embeddings, but only for the tokens that are not padding tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 127, 384])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have an embedding for each token\n",
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pooling, average the embeddings for each token into a single embedding vector\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize embeddings\n",
    "# something we should explain... its often taken for granted or glossed over\n",
    "# but the embeddings are normalized to unit length\n",
    "normalized_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting latentsae\n",
      "  Using cached latentsae-0.1.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (2.1.1)\n",
      "Collecting wandb (from latentsae)\n",
      "  Downloading wandb-0.18.3-py3-none-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting optuna (from latentsae)\n",
      "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pyyaml in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (6.0.2)\n",
      "Collecting datasets (from latentsae)\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pyarrow (from latentsae)\n",
      "  Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tqdm in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (4.66.5)\n",
      "Collecting dataclasses (from latentsae)\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting simple-parsing (from latentsae)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting einops (from latentsae)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: safetensors in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (0.4.5)\n",
      "Collecting accelerate (from latentsae)\n",
      "  Downloading accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (4.45.1)\n",
      "Collecting bitsandbytes (from latentsae)\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: huggingface-hub in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from latentsae) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from accelerate->latentsae) (24.1)\n",
      "Requirement already satisfied: psutil in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from accelerate->latentsae) (6.0.0)\n",
      "Requirement already satisfied: filelock in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from huggingface-hub->latentsae) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from huggingface-hub->latentsae) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from huggingface-hub->latentsae) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from huggingface-hub->latentsae) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from torch->latentsae) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from torch->latentsae) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from torch->latentsae) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from torch->latentsae) (75.1.0)\n",
      "Requirement already satisfied: scipy in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from bitsandbytes->latentsae) (1.14.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->latentsae)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->latentsae)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets->latentsae)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->latentsae)\n",
      "  Using cached multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->latentsae)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->latentsae)\n",
      "  Downloading aiohttp-3.10.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna->latentsae)\n",
      "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna->latentsae)\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna->latentsae)\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->latentsae)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from transformers->latentsae) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from transformers->latentsae) (0.20.0)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb->latentsae)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->latentsae)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->latentsae)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from wandb->latentsae) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb->latentsae)\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->latentsae)\n",
      "  Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb->latentsae)\n",
      "  Using cached setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl.metadata (9.9 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna->latentsae)\n",
      "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb->latentsae) (1.16.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->latentsae)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->latentsae)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets->latentsae)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->latentsae)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->latentsae)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets->latentsae)\n",
      "  Downloading yarl-1.14.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->latentsae)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from requests->huggingface-hub->latentsae) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from requests->huggingface-hub->latentsae) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from requests->huggingface-hub->latentsae) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from requests->huggingface-hub->latentsae) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from jinja2->torch->latentsae) (2.1.5)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets->latentsae)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from pandas->datasets->latentsae) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->latentsae)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->latentsae)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/enjalot/code/touch-tokens/ttenv/lib/python3.12/site-packages (from sympy->torch->latentsae) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->latentsae)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets->latentsae)\n",
      "  Downloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Using cached latentsae-0.1.0-py3-none-any.whl (19 kB)\n",
      "Downloading accelerate-1.0.0-py3-none-any.whl (330 kB)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Using cached pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
      "Downloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "Downloading wandb-0.18.3-py3-none-macosx_11_0_arm64.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.9-cp312-cp312-macosx_11_0_arm64.whl (391 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.28.2-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\n",
      "Downloading SQLAlchemy-2.0.35-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Using cached setproctitle-1.3.3-cp312-cp312-macosx_10_9_universal2.whl (16 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.14.0-cp312-cp312-macosx_11_0_arm64.whl (85 kB)\n",
      "Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "Downloading propcache-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, dataclasses, xxhash, tzdata, sqlalchemy, smmap, setproctitle, sentry-sdk, pyarrow, protobuf, propcache, multidict, Mako, fsspec, frozenlist, einops, docstring-parser, docker-pycreds, dill, colorlog, click, attrs, aiohappyeyeballs, yarl, simple-parsing, pandas, multiprocess, gitdb, bitsandbytes, alembic, aiosignal, optuna, gitpython, aiohttp, accelerate, wandb, datasets, latentsae\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed Mako-1.3.5 accelerate-1.0.0 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 alembic-1.13.3 attrs-24.2.0 bitsandbytes-0.42.0 click-8.1.7 colorlog-6.8.2 dataclasses-0.6 datasets-3.0.1 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.16 einops-0.8.0 frozenlist-1.4.1 fsspec-2024.6.1 gitdb-4.0.11 gitpython-3.1.43 latentsae-0.1.0 multidict-6.1.0 multiprocess-0.70.16 optuna-4.0.0 pandas-2.2.3 propcache-0.2.0 protobuf-5.28.2 pyarrow-17.0.0 pytz-2024.2 sentry-sdk-2.16.0 setproctitle-1.3.3 simple-parsing-0.1.6 smmap-5.0.1 sqlalchemy-2.0.35 tzdata-2024.2 wandb-0.18.3 xxhash-3.5.0 yarl-1.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install latentsae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latentsae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentsae.sae import Sae\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 18040.02it/s]\n",
      "Dropping extra args {'signed': False}\n"
     ]
    }
   ],
   "source": [
    "sae_model = Sae.load_from_hub(\"enjalot/sae-nomic-text-v1.5-FineWeb-edu-100BT\", \"64_32\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some metadata for the SAE to help us describe features\n",
    "sae_meta = json.load(open(\"/Users/enjalot/code/latent-taxonomy/web/public/models/NOMIC_FWEDU_25k/metadata.json\"))\n",
    "sae_features = pd.read_parquet(\"/Users/enjalot/code/latent-taxonomy/web/public/models/NOMIC_FWEDU_25k/features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "emb_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = emb_model.encode(inputs, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.15869600e-02,  2.08352990e-02, -1.92254588e-01,\n",
       "        -9.17355716e-02,  2.72580553e-02,  6.38516992e-02,\n",
       "        -1.85686518e-02,  1.10491365e-02,  1.82607630e-03,\n",
       "         2.33820500e-03, -5.76118343e-02, -1.79362893e-02,\n",
       "         5.58326654e-02,  7.99097195e-02,  2.99598444e-02,\n",
       "         1.22380489e-02,  5.03575206e-02, -3.67829576e-02,\n",
       "        -1.23029295e-02,  2.11170670e-02, -3.35725583e-02,\n",
       "        -3.16633433e-02, -6.66434243e-02, -1.86119997e-03,\n",
       "         1.27506945e-02,  4.17836681e-02, -8.43166038e-02,\n",
       "        -1.67221650e-02, -7.96173289e-02,  8.91472865e-03,\n",
       "         4.86480780e-02, -2.93906741e-02, -7.05600679e-02,\n",
       "        -1.38342381e-03, -1.24853812e-02, -5.06924242e-02,\n",
       "         3.19077075e-03, -5.24868490e-04,  4.56509143e-02,\n",
       "         2.04044469e-02,  2.90306266e-02, -8.74189020e-04,\n",
       "         1.96447577e-02,  3.89255285e-02,  9.87223089e-02,\n",
       "        -4.23938222e-03,  1.17774466e-02,  3.99901979e-02,\n",
       "        -1.29936016e-04, -2.83545768e-03,  6.14176355e-02,\n",
       "        -7.35232979e-02, -3.24940421e-02, -8.40068385e-02,\n",
       "         1.00248717e-01,  3.56897861e-02,  5.84363453e-02,\n",
       "        -5.06014451e-02,  6.53078198e-04,  2.65810229e-02,\n",
       "         1.33995950e-01,  9.54630598e-03,  3.73504311e-02,\n",
       "        -1.39792943e-02,  2.42329165e-02, -1.74171161e-02,\n",
       "        -1.73884463e-02, -1.89668988e-03,  5.28644025e-02,\n",
       "        -8.00051391e-02,  4.93717454e-02, -2.49731597e-02,\n",
       "         2.32382026e-02,  3.20653357e-02,  6.55121822e-03,\n",
       "        -1.12684723e-03,  2.44373921e-02, -1.21862143e-02,\n",
       "         6.38972316e-03,  3.56767774e-02,  1.19489050e-02,\n",
       "        -1.51260644e-02,  4.63665724e-02,  2.09215749e-02,\n",
       "         8.89742468e-03, -3.91343459e-02,  2.33693086e-02,\n",
       "         1.60808433e-02, -7.87324384e-02,  1.47882635e-02,\n",
       "         4.50053625e-02, -7.09056249e-03,  2.97373906e-02,\n",
       "        -5.75693185e-03, -2.81812251e-03, -5.26057323e-03,\n",
       "         1.51983732e-02, -8.25067051e-03, -7.67105594e-02,\n",
       "        -4.16712165e-02, -4.25740182e-02, -6.03507124e-02,\n",
       "        -1.25730485e-02, -3.28484997e-02,  7.53734112e-02,\n",
       "         2.51737460e-02,  1.23197620e-03, -1.16581703e-02,\n",
       "        -6.88811615e-02, -5.23498328e-03, -4.37813252e-02,\n",
       "        -4.32085246e-03, -3.18466090e-02, -4.49194275e-02,\n",
       "        -8.23900569e-03,  1.46598304e-02,  4.18929048e-02,\n",
       "        -2.14590654e-02,  2.54498459e-02,  4.21781233e-03,\n",
       "         5.48934238e-03,  2.23845877e-02, -9.64313187e-03,\n",
       "         3.57137620e-02,  4.42963503e-02,  3.83585766e-02,\n",
       "        -1.70671996e-02, -4.44175967e-04, -2.12901738e-02,\n",
       "        -3.50996964e-02, -1.08822966e-02,  9.43105016e-03,\n",
       "        -1.46801565e-02,  4.80954014e-02, -1.46067627e-02,\n",
       "         2.96481196e-02,  6.57579442e-03, -3.22249159e-02,\n",
       "        -4.61724075e-03, -2.95506399e-02,  1.84565447e-02,\n",
       "        -1.74601823e-02,  3.74471024e-03, -2.77657956e-02,\n",
       "        -2.91409642e-02,  2.86528654e-02, -2.52174605e-02,\n",
       "        -3.08530722e-02, -4.13319916e-02, -1.63052157e-02,\n",
       "         2.55030859e-03, -3.53011303e-03,  3.55033353e-02,\n",
       "         4.75674309e-02,  9.56790405e-04, -4.50726859e-02,\n",
       "         1.26052294e-02,  6.90310597e-02, -8.31805635e-03,\n",
       "         2.62299161e-02,  3.24817337e-02, -1.91647206e-02,\n",
       "        -3.64361741e-02,  3.00531890e-02,  2.86441557e-02,\n",
       "         7.75939086e-03,  1.52973868e-02,  2.25754175e-02,\n",
       "         1.83870215e-02,  5.78593742e-03, -2.47334074e-02,\n",
       "        -2.73758303e-02, -4.65865135e-02, -5.35484850e-02,\n",
       "         1.14424322e-02, -4.25604954e-02,  4.10546884e-02,\n",
       "        -3.06125749e-02,  1.61115581e-03, -4.40562963e-02,\n",
       "         4.50361036e-02, -6.07886016e-02,  1.20750163e-02,\n",
       "        -2.20816452e-02,  4.14507370e-03, -6.41343463e-03,\n",
       "         1.78074501e-02,  7.44770281e-03, -5.41833155e-02,\n",
       "        -2.32930873e-02, -4.14747596e-02,  1.89153589e-02,\n",
       "        -1.73341986e-02,  1.90872382e-02, -9.17196088e-03,\n",
       "        -2.02082191e-03,  1.25234295e-02,  6.75642956e-03,\n",
       "         2.69178171e-02,  3.52034159e-02,  3.65375131e-02,\n",
       "         1.46642653e-02, -1.74881646e-03,  2.17530760e-03,\n",
       "        -4.14041728e-02,  1.01864547e-01,  1.10410256e-02,\n",
       "         4.87456992e-02, -2.78738756e-02, -4.08013761e-02,\n",
       "         8.16613361e-02, -8.77589639e-03, -2.76295263e-02,\n",
       "        -2.40090347e-04, -4.38527251e-03,  7.84819666e-03,\n",
       "        -1.14128571e-02, -8.34428240e-03, -5.02058677e-02,\n",
       "         3.72023247e-02,  1.70438383e-02,  6.14175312e-02,\n",
       "         6.38740556e-03, -3.74496914e-02,  3.06935590e-02,\n",
       "         1.20471790e-02, -4.79363091e-02, -4.24201041e-02,\n",
       "        -3.91113199e-02, -2.14738585e-02, -5.86479567e-02,\n",
       "        -1.03944372e-02,  5.97206503e-02,  6.36665151e-02,\n",
       "        -4.78061661e-02,  2.00036578e-02, -6.09934935e-03,\n",
       "         4.70922999e-02, -1.06145656e-02, -7.36028655e-03,\n",
       "         1.73497982e-02,  4.18450236e-02,  2.89613120e-02,\n",
       "        -3.37003842e-02, -6.67577311e-02,  1.38460109e-02,\n",
       "        -1.11557320e-02, -5.49719296e-02, -4.32024486e-02,\n",
       "         3.43066044e-02,  3.29468995e-02,  3.22408453e-02,\n",
       "         2.07896959e-02,  1.83854881e-03,  1.22022619e-02,\n",
       "         1.31248431e-02, -4.00087312e-02, -1.60783082e-02,\n",
       "        -2.67321058e-02,  9.27538145e-03,  2.48440243e-02,\n",
       "        -3.46141681e-02, -3.08446325e-02, -5.33494987e-02,\n",
       "        -4.28153239e-02,  8.31687078e-03, -3.80836204e-02,\n",
       "         2.38172878e-02,  4.33758534e-02, -6.23959787e-02,\n",
       "         5.68429241e-03,  2.92210747e-02,  2.58471482e-02,\n",
       "         1.44164134e-02, -1.45094171e-02,  4.66100872e-03,\n",
       "         4.85548982e-04, -3.25432010e-02, -1.96943954e-02,\n",
       "         2.06899624e-02, -2.69969609e-02,  6.51940657e-03,\n",
       "        -1.91236362e-02,  1.39247654e-02, -2.88982596e-03,\n",
       "        -9.76335909e-03, -2.25418643e-03,  4.76677381e-02,\n",
       "        -6.48364599e-04, -3.63155492e-02,  3.42100337e-02,\n",
       "         3.06712259e-02, -1.76371075e-02,  4.42716777e-02,\n",
       "         1.69820134e-02,  3.86974849e-02,  8.29061344e-02,\n",
       "        -9.88012273e-03, -6.55489266e-02, -2.72311065e-02,\n",
       "         1.56250857e-02,  2.09793728e-03,  2.99804993e-02,\n",
       "         7.37926662e-02, -4.78970446e-02, -2.46321270e-03,\n",
       "         5.58089884e-03, -3.15846540e-02,  5.84294274e-02,\n",
       "        -3.38436775e-02, -2.85827257e-02,  2.31081098e-02,\n",
       "         2.11150851e-02,  6.06537983e-02, -6.41235709e-02,\n",
       "         6.59807175e-02,  9.16195754e-03, -1.30141964e-02,\n",
       "         6.03469312e-02, -1.80598479e-02, -1.75579488e-02,\n",
       "         8.21902696e-03, -3.55717773e-03, -6.08454086e-02,\n",
       "         2.35399790e-02,  1.19842421e-02,  2.80184355e-02,\n",
       "         6.16194159e-02, -1.39867468e-02,  1.76807232e-02,\n",
       "        -4.52285856e-02, -7.89312553e-03, -2.15332303e-03,\n",
       "        -4.20849435e-02, -2.12530196e-02,  5.54235205e-02,\n",
       "        -2.12465711e-02,  5.63332438e-03, -4.83978586e-03,\n",
       "        -4.80844593e-03,  5.55436723e-02, -1.95791330e-02,\n",
       "         5.10372873e-03, -3.86674963e-02,  1.77229065e-02,\n",
       "         8.86219321e-04, -5.01326695e-02, -2.14580018e-02,\n",
       "         2.34061964e-02, -1.39075750e-02, -6.57505682e-03,\n",
       "        -2.85914522e-02, -4.07036245e-02, -1.95260253e-02,\n",
       "         6.28910027e-03,  2.07277685e-02, -2.24415585e-02,\n",
       "         4.78633158e-02, -2.14424226e-02, -3.50417313e-03,\n",
       "         2.23258324e-02, -4.21145058e-04,  9.13150515e-03,\n",
       "        -7.98954368e-02, -5.02402987e-03, -4.63830726e-03,\n",
       "         3.95042123e-04,  7.04521080e-03,  3.44997421e-02,\n",
       "         3.82473096e-02, -4.99492884e-02, -1.85606685e-02,\n",
       "        -1.71535686e-02,  6.60484731e-02,  2.61613540e-02,\n",
       "        -8.24347697e-03, -7.69497780e-03, -5.71690165e-02,\n",
       "        -4.83563915e-03,  3.03585478e-03, -3.72641124e-02,\n",
       "        -4.30781953e-02, -2.13363040e-02,  2.49956567e-02,\n",
       "         6.70161694e-02,  1.03266400e-04,  4.11926885e-04,\n",
       "        -2.33389400e-02,  2.82345130e-03, -1.81624070e-02,\n",
       "         2.30860170e-02, -4.83658537e-02, -4.60254177e-02,\n",
       "        -7.51884002e-03,  6.03620745e-02, -4.38287966e-02,\n",
       "        -3.29527142e-03, -9.93711408e-03, -3.62030230e-02,\n",
       "         2.95194145e-02, -1.33256335e-02, -2.20669899e-02,\n",
       "         2.31001228e-02, -3.84052546e-04, -2.19634101e-02,\n",
       "         4.06001471e-02,  9.80677363e-03, -2.08991971e-02,\n",
       "         2.50805952e-02, -1.73866041e-02,  1.65785439e-02,\n",
       "         1.51927555e-02,  1.66128296e-03, -4.14051786e-02,\n",
       "        -1.98239274e-02,  2.79530380e-02,  9.69408900e-02,\n",
       "         2.70298645e-02, -1.23182470e-02,  1.11094192e-02,\n",
       "         6.03772374e-03,  2.99499184e-02, -4.60556075e-02,\n",
       "        -9.57824872e-04,  1.81536973e-02,  2.29283944e-02,\n",
       "         4.06302325e-02,  2.67273504e-02,  5.34543209e-03,\n",
       "        -1.96062997e-02,  2.91594188e-03, -2.29383800e-02,\n",
       "         3.24937738e-02,  5.05034346e-03,  5.24876080e-02,\n",
       "         4.74005677e-02,  4.75028828e-02, -8.89608543e-03,\n",
       "         6.01507388e-02,  1.23478934e-01,  8.59045461e-02,\n",
       "        -4.48672175e-02,  8.74277554e-04,  3.24505083e-02,\n",
       "         3.80239934e-02,  5.68661131e-02,  5.12407534e-02,\n",
       "        -3.02158762e-02, -1.68356132e-02, -9.58582666e-03,\n",
       "        -3.50466184e-02, -1.45977102e-02,  1.85086355e-02,\n",
       "         2.53118891e-02,  5.16476855e-02, -3.85162532e-02,\n",
       "        -4.11650389e-02,  2.22138576e-02,  1.56476256e-02,\n",
       "         4.30312306e-02,  1.14343809e-02,  1.46413902e-02,\n",
       "        -1.56778805e-02,  2.77574901e-02,  8.77693575e-03,\n",
       "        -4.83674370e-03, -2.65465397e-02, -1.49078164e-02,\n",
       "         2.97164731e-03,  5.25702462e-02, -2.55980939e-02,\n",
       "         2.45590098e-02, -8.60907603e-03, -4.73570414e-02,\n",
       "         3.42687517e-02, -2.07945537e-02, -2.48335674e-02,\n",
       "        -4.39473568e-03,  4.55036908e-02,  9.77286324e-02,\n",
       "         1.55153079e-02, -7.10430648e-03, -5.53909875e-02,\n",
       "        -3.88922356e-02,  2.93912310e-02, -5.27701806e-03,\n",
       "         2.59717181e-02, -8.66380148e-03,  1.48598887e-02,\n",
       "        -7.91606680e-03,  2.64055878e-02, -2.32123435e-02,\n",
       "         1.63506567e-02, -1.40911331e-02,  2.72065226e-04,\n",
       "        -3.49676944e-02,  1.48319742e-02,  3.87587398e-02,\n",
       "         6.85217902e-02, -2.78696921e-02,  2.48909462e-02,\n",
       "         7.46034309e-02, -2.12370716e-02,  1.56131224e-03,\n",
       "         2.15840675e-02, -5.46549854e-04, -5.04928902e-02,\n",
       "        -2.37599388e-02, -2.32612696e-02,  5.16245104e-02,\n",
       "        -4.22339626e-02, -1.70110334e-02, -4.29604426e-02,\n",
       "         6.33516833e-02,  8.84817243e-02, -3.28803696e-02,\n",
       "        -4.63610003e-03,  1.04608713e-02,  1.75270420e-02,\n",
       "         3.17040235e-02, -1.81968859e-03, -4.18275222e-02,\n",
       "         5.42479753e-03,  1.89292862e-03, -4.74670231e-02,\n",
       "         1.11604109e-02, -4.37608862e-04, -2.33020987e-02,\n",
       "         1.35086551e-02,  1.76740699e-02,  2.83512678e-02,\n",
       "        -2.07898859e-02, -4.82958630e-02, -1.76603738e-02,\n",
       "         1.94272175e-02, -1.11242030e-02, -4.59359735e-02,\n",
       "         4.53060828e-02,  1.99512597e-02,  6.86953366e-02,\n",
       "        -4.17195745e-02,  3.32621485e-02, -2.87540089e-02,\n",
       "         6.05658107e-02, -2.17704009e-02,  1.30132940e-02,\n",
       "        -3.40735763e-02,  4.78327572e-02,  2.71860100e-02,\n",
       "         8.06219527e-04,  3.82664129e-02, -3.11346743e-02,\n",
       "        -9.83964000e-03,  2.32003964e-02,  1.44602461e-02,\n",
       "        -4.49636159e-03, -4.18706164e-02, -3.10388021e-02,\n",
       "        -1.05530173e-02, -2.19568629e-02,  1.65515468e-02,\n",
       "        -8.11824482e-03,  7.74903372e-02,  2.35543177e-02,\n",
       "         3.74464616e-02,  6.30246848e-03, -2.27035731e-02,\n",
       "        -4.86926101e-02, -5.24616800e-03,  1.85379833e-02,\n",
       "         5.60642332e-02, -6.35937378e-02, -3.76330526e-03,\n",
       "        -1.71067007e-02, -1.23642059e-02,  2.53878497e-02,\n",
       "         1.88133270e-02,  1.84733747e-03, -7.52416402e-02,\n",
       "        -7.73373768e-02,  3.92301679e-02, -7.36158434e-03,\n",
       "         4.43607755e-02,  9.72295739e-03, -2.49294806e-02,\n",
       "        -3.52308787e-02, -4.40602051e-03,  5.91036631e-03,\n",
       "         7.93312788e-02,  6.71142526e-03,  2.61757001e-02,\n",
       "        -6.21097013e-02,  5.99032640e-03, -2.69242227e-02,\n",
       "        -1.97307169e-02,  1.68328453e-03,  1.13687925e-02,\n",
       "        -4.20425385e-02, -2.01100856e-02, -3.70931402e-02,\n",
       "        -1.99007224e-02,  4.33754660e-02,  9.99742225e-02,\n",
       "        -8.68899748e-02, -2.18903515e-02,  3.75329866e-03,\n",
       "        -4.06114501e-04, -4.58367132e-02,  3.06536648e-02,\n",
       "         6.43635821e-03,  3.11565101e-02, -2.09227558e-02,\n",
       "        -5.00383647e-03, -8.83002020e-03, -3.49505479e-03,\n",
       "        -3.68947014e-02,  7.12048337e-02, -2.77811531e-02,\n",
       "        -4.01173607e-02, -4.38554436e-02, -2.61166412e-02,\n",
       "        -7.20792189e-02,  8.91439319e-02, -4.50177081e-02,\n",
       "        -2.22618263e-02, -2.40545645e-02, -5.70279099e-02,\n",
       "        -2.34120078e-02, -1.38269300e-02,  4.34578434e-02,\n",
       "        -6.38691708e-02, -1.95964146e-02, -5.96830808e-02,\n",
       "         3.33208069e-02,  1.12260431e-02,  2.07916964e-02,\n",
       "        -4.64621745e-03, -4.40323493e-03, -3.65103818e-02,\n",
       "        -8.65217857e-03,  1.08874263e-02, -1.13637876e-02,\n",
       "         2.88296081e-02,  2.50264518e-02,  1.43848713e-02,\n",
       "        -1.53625151e-02,  1.79559644e-02, -8.24069977e-03,\n",
       "         4.61093336e-02, -4.25389446e-02,  8.06304440e-02,\n",
       "         9.20380801e-02,  5.10025360e-02,  5.13816252e-02,\n",
       "         2.04726867e-03,  4.95456671e-03,  4.20105420e-02,\n",
       "        -4.11403850e-02, -6.15916885e-02, -1.24077033e-02,\n",
       "        -3.04420548e-03, -1.31806545e-03, -4.77993451e-02,\n",
       "         1.38950767e-02,  9.65946168e-03,  3.30326669e-02,\n",
       "        -1.60278715e-02, -7.23284557e-02, -3.06640323e-02,\n",
       "         6.91762054e-03, -1.92853753e-02,  5.92245162e-02,\n",
       "        -2.71670017e-02, -3.72589938e-02, -8.63759592e-03,\n",
       "         6.38960814e-03,  3.11655551e-02, -5.27183898e-03,\n",
       "         4.38081548e-02, -3.91792506e-02, -5.80952130e-02,\n",
       "        -2.34496575e-02,  2.83404980e-02, -2.80732866e-02,\n",
       "        -5.31446561e-03, -2.02536173e-02,  1.74411740e-02,\n",
       "         1.30193261e-02, -3.46963964e-02, -4.18074839e-02,\n",
       "        -3.57503630e-02, -9.15070344e-03,  5.49212322e-02,\n",
       "         6.78422255e-03, -2.71499678e-02, -7.03498302e-03,\n",
       "         3.07477862e-02, -2.28369329e-02,  1.31805958e-02,\n",
       "        -1.63904056e-02, -4.48312648e-02,  1.01634692e-02,\n",
       "         1.79708228e-02,  4.14686464e-02, -3.26995626e-02,\n",
       "        -1.87338865e-03,  2.16841511e-02, -9.61908884e-03,\n",
       "         2.44828407e-02,  2.22276337e-02, -5.27973706e-03,\n",
       "         5.29840589e-03,  1.94902513e-02,  2.07196586e-02,\n",
       "         7.78149739e-02, -3.31915766e-02, -3.07760742e-02,\n",
       "         1.12182007e-03, -6.09164983e-02,  5.22160232e-02,\n",
       "         6.31632209e-02, -6.81254864e-02, -6.35316595e-02,\n",
       "        -5.15942164e-02,  3.23686600e-02,  1.93122048e-02,\n",
       "         5.49308360e-02, -1.71520337e-02,  7.43157510e-03,\n",
       "         1.12109743e-02,  4.05464172e-02, -2.58505289e-02,\n",
       "        -3.58778387e-02, -1.56595446e-02, -1.84451304e-02,\n",
       "         1.10444166e-02, -1.49923963e-02, -2.70565450e-02,\n",
       "        -6.73161298e-02, -3.72557640e-02, -2.91525684e-02,\n",
       "        -6.10551424e-03,  2.00051777e-02,  4.06360738e-02,\n",
       "         2.53397459e-03, -6.47402182e-02, -8.42822157e-03,\n",
       "         4.75297309e-03,  9.09062382e-03, -3.41077484e-02,\n",
       "         8.82830285e-03, -1.68617014e-02, -1.81147177e-02,\n",
       "        -1.12557784e-02, -4.43843659e-03, -7.47765228e-03,\n",
       "        -1.14518241e-03, -7.05177011e-03,  7.18385875e-02,\n",
       "        -7.70008052e-03, -3.43220234e-02, -4.54344526e-02,\n",
       "         4.76295389e-02, -2.67130900e-02, -2.56162528e-02,\n",
       "         1.78659998e-03,  1.45221269e-02,  2.55972659e-03]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_latents = sae_model.encode(torch.from_numpy(embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderOutput(top_acts=tensor([[0.1960, 0.1681, 0.1379, 0.1321, 0.1249, 0.1210, 0.1190, 0.1176, 0.0991,\n",
       "         0.0820, 0.0775, 0.0743, 0.0729, 0.0710, 0.0671, 0.0658, 0.0615, 0.0602,\n",
       "         0.0601, 0.0575, 0.0570, 0.0561, 0.0533, 0.0521, 0.0519, 0.0501, 0.0463,\n",
       "         0.0461, 0.0454, 0.0439, 0.0436, 0.0430, 0.0418, 0.0408, 0.0404, 0.0395,\n",
       "         0.0394, 0.0384, 0.0382, 0.0376, 0.0373, 0.0372, 0.0365, 0.0360, 0.0358,\n",
       "         0.0358, 0.0356, 0.0349, 0.0340, 0.0337, 0.0331, 0.0330, 0.0328, 0.0325,\n",
       "         0.0323, 0.0317, 0.0314, 0.0313, 0.0307, 0.0297, 0.0295, 0.0291, 0.0289,\n",
       "         0.0289]], grad_fn=<TopkBackward0>), top_indices=tensor([[ 3020,  8990,   304, 14433, 12363, 10739, 17432, 13030, 21919, 17790,\n",
       "          9132,  1309, 23332,  5392, 15241,  5676,  2340, 13476, 18302,  4207,\n",
       "          3863, 18703, 12129, 17693,  8104,  1068, 21838, 17469,  2570, 22818,\n",
       "          6981, 14476, 22688,  5527, 17265,  2454,  6674,  9934, 20069, 18553,\n",
       "         19690,   928,  2595, 19736,  1893, 13771, 14621, 23250, 11731,  2119,\n",
       "         15167, 18844, 14910,   261, 13129, 17822, 23061, 18463, 19383, 13283,\n",
       "          1251,  3596, 10523,   323]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3020)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index of the top \"feature\"\n",
    "sae_latents.top_indices[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature                                                   304\n",
       "max_activation                                        0.33558\n",
       "x                                                    0.575523\n",
       "y                                                    0.231955\n",
       "top10_x                                             -0.682711\n",
       "top10_y                                               0.12703\n",
       "label             themes of destruction suffering and despair\n",
       "order                                                0.296192\n",
       "Name: 304, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the sae metadata we have for the top feature\n",
    "sae_features.iloc[sae_latents.top_indices[0][0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['publishing and writing about literature',\n",
       " 'book review and analysis techniques',\n",
       " 'themes of destruction suffering and despair',\n",
       " 'supportive communication in addiction recovery contexts',\n",
       " 'prioritizing responsibilities over personal enjoyment',\n",
       " 'social change resilience and identity exploration',\n",
       " 'academic and scientific excellence in personal relationships',\n",
       " 'metaphorical and literal interpretations of falling']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the labels for the top 8 features\n",
    "top8 = sae_latents.top_indices[0][:8]\n",
    "top8_features = sae_features.iloc[top8]\n",
    "top8_features[\"label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
