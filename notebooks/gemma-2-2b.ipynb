{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x298491520>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some notes from SAE tutorial on gemmascope https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=12wF3f7o1Ni7\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# this is what you do if you only want inference (not training)\n",
    "# saves on memory usage\n",
    "torch.set_grad_enabled(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# every transformer has a tokenizer, so we load the one for the model we want to use\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "# this downloads the model (or loads it from disk cache)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to study what's happening in the model when we run some input text through it\n",
    "input_text = \"hello, Yoda my name is\"\n",
    "# the first step is to tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,  17534, 235269, 146433,    970,   1503,    603]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 2: \t\t<bos>\n",
      "Token 17534: \t\thello\n",
      "Token 235269: \t\t,\n",
      "Token 146433: \t\t Yoda\n",
      "Token 970: \t\t my\n",
      "Token 1503: \t\t name\n",
      "Token 603: \t\t is\n"
     ]
    }
   ],
   "source": [
    "for t in input_ids['input_ids'][0]:\n",
    "  print(f\"Token {t}: \\t\\t{tokenizer.decode(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tokens 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([     2,  17534, 235269, 146433,    970,   1503,    603, 146433,    578])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can run our model. Let's generate just 2 more tokens for now\n",
    "outputs = model.generate(**input_ids, max_new_tokens=2)\n",
    "print(\"output tokens\", len(outputs[0]))\n",
    "outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, Yoda my name is Yoda and'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to turn the output tokens back into text (the output includes the input)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get hidden states for the input, we need to run the model again with the generated sequence\n",
    "hidden_output = model(**input_ids, output_hidden_states=True)\n",
    "\n",
    "# Access hidden states\n",
    "hidden_states = hidden_output.hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens 7\n",
      "hidden states (layers) 27\n",
      "hidden state shape torch.Size([1, 7, 2304])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"input tokens\", len(input_ids['input_ids'][0]))\n",
    "print(\"hidden states (layers)\", len(hidden_states))\n",
    "print(\"hidden state shape\", hidden_states[20].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-25): 26 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
      "          (rotary_emb): Gemma2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
      "  (_cache): HybridCache()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in config: 26\n",
      "Hidden size: 2304\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of layers in config: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  \"\"\"\n",
    "  This function allows us to gather activations for a specific layer on a model.\n",
    "  \n",
    "  Args:\n",
    "  - model: The model from which we want to gather activations.\n",
    "  - target_layer: The specific layer index for which we want to gather activations.\n",
    "  - inputs: The input data to be passed through the model.\n",
    "  \n",
    "  Returns:\n",
    "  - target_act: The activations of the specified layer.\n",
    "  \"\"\"\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  # we could also easily target the MLP layer\n",
    "  # handle = model.model.layers[target_layer].mlp.register_forward_hook(gather_mlp_output_hook)\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "  _ = model.forward(inputs)\n",
    "  handle.remove()\n",
    "  return target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 20th index is actually the 21st layer\n",
    "target_act = gather_residual_activations(model, 20, input_ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 2304])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9440,  1.7632, -2.0879,  ...,  1.6978, -2.0868, -0.0178],\n",
       "         [-7.1130,  2.3935, -0.3611,  ..., -0.0207,  4.9698,  2.0012],\n",
       "         [-8.1484,  0.2589, -0.5944,  ..., -1.2177,  2.7641,  2.0122],\n",
       "         ...,\n",
       "         [ 1.5289, -3.7196,  7.7939,  ..., -9.8385,  0.1159,  0.6954],\n",
       "         [-3.1761,  1.1008,  0.5456,  ..., -1.7524, -2.0917,  2.4893],\n",
       "         [ 0.9512, -2.4978, -0.4893,  ..., -4.7208, -5.6637, -0.7142]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9440,  1.7632, -2.0879,  ...,  1.6978, -2.0868, -0.0178],\n",
       "         [-7.1130,  2.3935, -0.3611,  ..., -0.0207,  4.9698,  2.0012],\n",
       "         [-8.1484,  0.2589, -0.5944,  ..., -1.2177,  2.7641,  2.0122],\n",
       "         ...,\n",
       "         [ 1.5289, -3.7196,  7.7939,  ..., -9.8385,  0.1159,  0.6954],\n",
       "         [-3.1761,  1.1008,  0.5456,  ..., -1.7524, -2.0917,  2.4893],\n",
       "         [ 0.9512, -2.4978, -0.4893,  ..., -4.7208, -5.6637, -0.7142]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the weights for the SAE we want to use\n",
    "# https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v) for k, v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class JumpReLUSAE(nn.Module):\n",
    "  def __init__(self, d_model, d_sae):\n",
    "    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "    # If you want to train your own SAEs then we recommend using blah\n",
    "    super().__init__()\n",
    "    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "    self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def encode(self, input_acts):\n",
    "    pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "    mask = (pre_acts > self.threshold)\n",
    "    acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "    return acts\n",
    "\n",
    "  def decode(self, acts):\n",
    "    return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, acts):\n",
    "    acts = self.encode(acts)\n",
    "    recon = self.decode(acts)\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['W_enc'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "reconstruction = sae.decode(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 16384])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 2304])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6631, 14956, 10299, 15449, 11302,  8564, 15449]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, inds = sae_acts.max(-1)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2028.7983,  122.3900,  107.6448,   90.2571,   89.6321,  102.8196,\n",
       "           42.7972]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 values shape: torch.Size([1, 7, 5])\n",
      "Top 5 indices shape: torch.Size([1, 7, 5])\n",
      "\n",
      "Top 5 values for first sequence item:\n",
      "tensor([2028.7983,  781.3959,  534.8594,  264.1917,  252.5279])\n",
      "\n",
      "Top 5 indices for first sequence item:\n",
      "tensor([ 6631,   743,  5052, 16057,  9479])\n"
     ]
    }
   ],
   "source": [
    "k = 5  # Change this to the number of top values you want\n",
    "values, indices = torch.topk(sae_acts, k, dim=-1)\n",
    "\n",
    "print(f\"Top {k} values shape: {values.shape}\")\n",
    "print(f\"Top {k} indices shape: {indices.shape}\")\n",
    "\n",
    "# Print the top k values and indices for the first sequence item\n",
    "print(f\"\\nTop {k} values for first sequence item:\")\n",
    "print(values[0, 0])\n",
    "print(f\"\\nTop {k} indices for first sequence item:\")\n",
    "print(indices[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/15449?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2984a8aa0>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=15449)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modify_layer_activation(model, target_layer, input_ids, sae, feature_index, modification_value, max_new_tokens):\n",
    "    \"\"\"\n",
    "    Modify the activation of a specific feature in a given layer.\n",
    "    \n",
    "    Args:\n",
    "    - model: The LLM model\n",
    "    - target_layer: The index of the layer to modify\n",
    "    - input_ids: The input token IDs\n",
    "    - sae: The Sparse Autoencoder\n",
    "    - feature_index: The index of the feature to modify\n",
    "    - modification_value: The value to add to the feature's activation\n",
    "    \n",
    "    Returns:\n",
    "    - modified_output: The model's output after modification\n",
    "    \"\"\"\n",
    "    def capture_and_modify_hook(module, inputs, outputs):\n",
    "        # Capture the original activation\n",
    "        original_act = outputs[0].detach()\n",
    "        \n",
    "        # Encode the activations using the SAE\n",
    "        sae_acts = sae.encode(original_act.to(torch.float32))\n",
    "        \n",
    "        # Modify the specific feature's activation\n",
    "        sae_acts[0, :, feature_index] += modification_value\n",
    "        \n",
    "        # Decode the modified activations\n",
    "        modified_act = sae.decode(sae_acts)\n",
    "        \n",
    "        # Return the modified activation\n",
    "        return (modified_act,) + outputs[1:]\n",
    "\n",
    "    # Register the hook\n",
    "    handle = model.model.layers[target_layer].register_forward_hook(capture_and_modify_hook)\n",
    "    \n",
    "    # Run the model with the modified activation\n",
    "    with torch.no_grad():\n",
    "        modified_output = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Remove the hook\n",
    "    handle.remove()\n",
    "    \n",
    "    return modified_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to study what's happening in the model when we run some input text through it\n",
    "input_text2 = \"Hello, my name is\"\n",
    "# the first step is to tokenize the input text\n",
    "input_ids2 = tokenizer(input_text2, return_tensors=\"pt\", add_special_tokens=True)\n",
    "max_new_tokens = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = model.generate(**input_ids2, max_new_tokens=max_new_tokens)\n",
    "generated_text2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, my name is Dr. David and I'm a professor of\n",
      "Modified text: Hello, my name is Luke Skywalker Skywalker Skywalker Skywalker Skywalker Skywalker Skywalker Skywalker Skywalker\n"
     ]
    }
   ],
   "source": [
    "target_layer = 20  # The layer you want to modify\n",
    "feature_index = 15449  # The feature index you want to modify\n",
    "modification_value = 1000.0  # The value to add to the feature's activation\n",
    "\n",
    "modified_output = modify_layer_activation(model, target_layer, input_ids2['input_ids'], sae, feature_index, modification_value, max_new_tokens)\n",
    "\n",
    "# Generate text from the modified output\n",
    "modified_text = tokenizer.decode(modified_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Original text:\", generated_text2)\n",
    "print(\"Modified text:\", modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
